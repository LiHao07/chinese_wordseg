{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 载入sogou语料库生成中文的单字向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "w2v_weight = np.load('w2v_weight_sogou.word.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f=open(\"word2idx_sogou.word.json\",\"r\")\n",
    "for line in f:\n",
    "    word2idx = json.loads(line)\n",
    "#f=open(\"idx2word_sogou.word.json\",\"r\")\n",
    "#for line in f:\n",
    "#    idx2word = json.loads(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练数据读取和转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import sys\n",
    "import math\n",
    "\n",
    "def character_tagging(input_file):\n",
    "    input_data = codecs.open(input_file, 'r', 'utf-8')\n",
    "    global train_sent\n",
    "    global train_y\n",
    "    global max_len\n",
    "    train_sent = []\n",
    "    train_y = []\n",
    "    max_len = 0\n",
    "    for line in input_data.readlines():\n",
    "        label_list = []\n",
    "        word_list = line.strip().split()\n",
    "       \n",
    "        tot = 0\n",
    "        for word in word_list:\n",
    "            tot += len(word)\n",
    "            if len(word) == 1:\n",
    "                label_list.append(1)\n",
    "            else:\n",
    "                label_list.extend([0 for w in range(len(word)-1)])\n",
    "                label_list.append(1)\n",
    "        strtmp = ''.join(word_list)\n",
    "        L = len(label_list)\n",
    "        t = max(L-seg_len, 0)\n",
    "        for i in range(0,t,seg_len//4):\n",
    "            if(i+seg_len>L-1):\n",
    "                end = L\n",
    "            else:\n",
    "                end = i+seg_len    \n",
    "            train_sent.append(strtmp[i:end])\n",
    "            train_y.append(label_list[i:end])\n",
    "        \n",
    "        if tot>max_len:\n",
    "            max_len = tot\n",
    "\n",
    "character_tagging(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519878\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sent))\n",
    "max_len = seg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_weight = np.concatenate([np.zeros((3,300)), w2v_weight], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_unk = []\n",
    "def sent2num(sentence):\n",
    "    global word2idx\n",
    "    global tot_unk\n",
    "    predict_word_num = []\n",
    "    for w in sentence:\n",
    "        if w in word2idx:\n",
    "            predict_word_num.append(word2idx[w])\n",
    "        else:\n",
    "            predict_word_num.append(word2idx['UNK'])\n",
    "            tot_unk.append(w)\n",
    "    return predict_word_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "for i in range(len(train_sent)):\n",
    "    train_x.append(sent2num(train_sent[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=max_len, padding='post', value=0)\n",
    "train_y = sequence.pad_sequences(train_y, maxlen=max_len, padding='post', value=0).reshape(-1,seg_len,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1519878, 32)\n",
      "(1519878, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Reshape, Flatten ,Dropout\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D,MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_x\n",
    "train_Y = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364993, 300)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "maxfeatures = w2v_weight.shape[0] # 词典大小\n",
    "print(w2v_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 32, 300)           109497900 \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 32, 64)            186880    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 32, 10)            650       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 10)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 32, 1)             11        \n",
      "=================================================================\n",
      "Total params: 109,685,441\n",
      "Trainable params: 109,685,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, TimeDistributed, Masking, MaxPool1D\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(maxfeatures, 300, weights=[w2v_weight], input_length=max_len, mask_zero=True, trainable=False)) \n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True,implementation=2, activation='relu'), merge_mode='sum'))\n",
    "model.add(TimeDistributed(Dense(10, activation='relu')))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', recall, precision, fbeta_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class EarlyStopping(Callback):\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.best_val_loss = -1\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if not self.params['do_validation']:\n",
    "            warnings.warn(\"Early stopping requires validation data!\", RuntimeWarning)\n",
    "\n",
    "        cur_val_loss = logs.get('val_fbeta_score')\n",
    "        #cur_val_loss = logs.get('val_loss')\n",
    "        if cur_val_loss > self.best_val_loss:\n",
    "            self.best_val_loss = cur_val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            if self.wait <= self.patience:\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Epoch %05d: early stopping\" % (epoch))\n",
    "                self.model.stop_training = True\n",
    "            self.wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config)) \n",
    "keras.backend.get_session().run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1519878, 32)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 109497900 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1504679 samples, validate on 15199 samples\n",
      "Epoch 1/100\n",
      "1504679/1504679 [==============================] - 243s 161us/step - loss: 0.2250 - acc: 0.9126 - recall: 0.9490 - precision: 0.9053 - fbeta_score: 0.9263 - val_loss: 0.1163 - val_acc: 0.9562 - val_recall: 0.9737 - val_precision: 0.9546 - val_fbeta_score: 0.9640\n",
      "Epoch 2/100\n",
      "1504679/1504679 [==============================] - 238s 158us/step - loss: 0.1103 - acc: 0.9606 - recall: 0.9832 - precision: 0.9499 - fbeta_score: 0.9663 - val_loss: 0.1048 - val_acc: 0.9654 - val_recall: 0.9790 - val_precision: 0.9642 - val_fbeta_score: 0.9715\n",
      "Epoch 3/100\n",
      "1504679/1504679 [==============================] - 239s 159us/step - loss: 0.0796 - acc: 0.9716 - recall: 0.9898 - precision: 0.9618 - fbeta_score: 0.9756 - val_loss: 0.1048 - val_acc: 0.9685 - val_recall: 0.9820 - val_precision: 0.9662 - val_fbeta_score: 0.9740\n",
      "Epoch 4/100\n",
      "1504679/1504679 [==============================] - 239s 159us/step - loss: 0.0648 - acc: 0.9770 - recall: 0.9927 - precision: 0.9681 - fbeta_score: 0.9802 - val_loss: 0.1226 - val_acc: 0.9709 - val_recall: 0.9758 - val_precision: 0.9759 - val_fbeta_score: 0.9758\n",
      "Epoch 5/100\n",
      "1504679/1504679 [==============================] - 239s 159us/step - loss: 0.0500 - acc: 0.9824 - recall: 0.9948 - precision: 0.9749 - fbeta_score: 0.9848 - val_loss: 0.1314 - val_acc: 0.9718 - val_recall: 0.9794 - val_precision: 0.9741 - val_fbeta_score: 0.9767\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "print(\"Train...\")\n",
    "earlystop = EarlyStopping(patience=0, verbose=5)\n",
    "result = model.fit(train_X, train_Y, batch_size=batch_size, epochs=100, \n",
    "          validation_split=0.01, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_data_aug_32_embedding.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用test文本进行预测，评估效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扬帆远东做与中国合作的先行\n"
     ]
    }
   ],
   "source": [
    "test_file = 'test.txt'\n",
    "with open(test_file,'r') as f:\n",
    "    lines = f.readlines()\n",
    "    test_texts = [''.join(list(line.strip())) for line in lines]\n",
    "print(test_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2char(x):\n",
    "    if(x>0.5):\n",
    "        return ' '\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def predict_num(input_txt, predict_prob):\n",
    "    result =  [w+num2char(l)  for w, l in zip(input_txt,predict_prob)]\n",
    "    return ''.join(result) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_len = []\n",
    "input_txt = []\n",
    "test_num = []\n",
    "\n",
    "for line in test_texts:\n",
    "    input_txt.append(line)\n",
    "    tmp = sent2num(line)\n",
    "    for i in range(0,len(tmp),seg_len):\n",
    "        if(i==len(tmp)):\n",
    "            break\n",
    "        if(i+seg_len<=len(tmp)):\n",
    "            end = i+seg_len\n",
    "        else:\n",
    "            end = len(tmp)\n",
    "        test_num.append(tmp[i:end])\n",
    "    test_len.append(len(tmp))\n",
    "test_num = sequence.pad_sequences(test_num, maxlen=max_len, padding='post')\n",
    "test_num = np.array(test_num)\n",
    "predict_prob = model.predict_proba(test_num)\n",
    "predict_lable = model.predict_classes(test_num).reshape(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = []\n",
    "z = int(0)\n",
    "for i in range(len(test_len)):\n",
    "    z = int(z)\n",
    "    output_line = predict_num(input_txt[i], predict_lable[z:z+test_len[i]])\n",
    "    z += ((test_len[i]+seg_len-1)//seg_len)*seg_len\n",
    "    test_output.append(output_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_output.txt','w') as f:\n",
    "    f.writelines(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 最终使用perl脚本检验F值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./score train.txt test.answer.txt test_output.txt > deep_score_dense_seg_50_data_aug_embedding.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测方式2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78784, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_len = []\n",
    "input_txt = []\n",
    "test_num = []\n",
    "\n",
    "for line in test_texts:\n",
    "    input_txt.append(line)\n",
    "    tmp = sent2num(line)\n",
    "    if(len(tmp)<seg_len):\n",
    "        test_num.append(tmp)\n",
    "    else:\n",
    "        for i in range(0,len(tmp)-seg_len+1):\n",
    "            test_num.append(tmp[i:i+seg_len])\n",
    "    test_len.append(len(tmp))\n",
    "test_num = sequence.pad_sequences(test_num, maxlen=seg_len, padding='post')\n",
    "test_num = np.array(test_num)\n",
    "print(test_num.shape)\n",
    "predict_prob = model.predict_proba(test_num)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = []\n",
    "z = 0\n",
    "for i in range(len(test_len)):\n",
    "    l = test_len[i]\n",
    "    if(l<seg_len):\n",
    "        tmp = predict_num(input_txt[i],predict_prob[z])\n",
    "        test_output.append(tmp)\n",
    "        z+=1\n",
    "    else:\n",
    "        tmp = [[]for j in range(l)]\n",
    "        for j in range(0,l-seg_len+1):\n",
    "            for k in range(seg_len):\n",
    "                tmp[j+k].append(float(predict_prob[z+j][k]))\n",
    "        for j in range(l):\n",
    "            tmp[j]=sum(tmp[j])/len(tmp[j])\n",
    "        tmp = predict_num(input_txt[i], tmp)\n",
    "        test_output.append(tmp)\n",
    "        z+=l-seg_len+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_output.txt','w') as f:\n",
    "    f.writelines(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./score train.txt test.answer.txt test_output.txt > deep_score_dense_seg_50_data_aug_embedding.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
